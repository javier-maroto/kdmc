\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


% Added packages
\usepackage{cite}  
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

% Latex math commands
\newcommand{\Ls}{\mathcal{L}}
\def\sP{{\mathbb{P}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%\title{SafeAMC: Adversarial training for robust modulation recognition models}

%\name{Javier Maroto\thanks{This work has been sponsored by armasuisse Science and Technology with the project ROBIN (project code Aramis 047-22).}$^{\star}$ \qquad Gérôme Bovet$^{\dagger}$ \qquad Pascal Frossard$^{\star}$}
  
%\address{$^{\star}$ EPFL, Switzerland \\
%  $^{\dagger}$armasuisse Science\&Technology, Cyber-Defence Campus, Switzerland}
  
\title{ML distillation
\thanks{This work has been sponsored by armasuisse Science and Technology with the project ARNO (project code ???).}
}

\author{\IEEEauthorblockN{Javier Maroto}
\IEEEauthorblockA{\textit{Signal Processing Laboratory (LTS4)} \\ \textit{EPFL, Switzerland}}
\and
\IEEEauthorblockN{Gérôme Bovet}
\IEEEauthorblockA{\textit{armasuisse Science\&Technology} \\
\textit{Cyber-Defence Campus, Switzerland}}
\and
\IEEEauthorblockN{Pascal Frossard}
\IEEEauthorblockA{\textit{Signal Processing Laboratory (LTS4)} \\ \textit{EPFL, Switzerland}}}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\begin{IEEEkeywords}
\end{IEEEkeywords}

\section{Introduction}

% 

Deep learning \cite{goodfellow2016deep}

\section{Maximum likelihood}

Normally, to perform well on automatic modulation classification (AMC), neural networks are trained to infer the modulation used in transmission from the signal received. Thus, the network will be optimized to be as confident and accurate as possible on the training set. However, while enforcing the network to accurately solve the task is feasible for signals with high SNR, on noisier signals this may no longer the case. The reason is that there could be a non-negligible probability that the signal received was generated from a different modulation than the one we are enforcing the model to predict. In some extreme cases, the noise can even make the task impossible to solve, where the signal received could have been generated from any of multiple modulations with similar probability.

In many other tasks, these probabilities would be difficult or impossible to obtain. However, in AMC, there is an optimal method that gives us these probabilities with perfect accuracy: Maximum Likelihood (ML). Despite its optimal performance, neural networks are still preferred over ML for a simple reason: ML does not perform well when we do not have full knowledge of the channel effects on the signal. In fact, even if we were to estimate the channel, any small deviation from the real channel parameters has a major effect on the ML performance. Moreover, incorporating this uncertainty in the ML formulation makes it computationally unfeasible.

Based on these limitations, we propose a hybrid approach. We will use a neural network for inference, but we will train it with the probabilities given by maximum likelihood. This is feasible when we consider that we can have full knowledge of the channel conditions by generating the training samples ourselves. Moreover, after training the neural network, ML is no longer needed. Thus allowing our model to be usable under unknown channel conditions, which is typical in real AMC scenarios.


\section{Knowledge distillation}

\section{Experiments}

We propose to evaluate our methodology on scenarios with different levels of non-coherence. This way, we can verify if the probabilities generated by maximum likelihood in a coherent scenario are useful to train the model even if it is tested on varying channel and transmitter conditions. 
For all the experiments the task will be the same. We will train the neural network to be able to distinguish between the following digital modulations: BPSK, QPSK, 8/16/32/64/128/256-PSK, PAM4, 16/32/64/128/256-QAM. This is consistent with other experimental settings used in the field \cite{}.  % Avoided highlighting that analog modulations are not used (not viable with our methodology, would have to be ignored)
For all settings, we use 200KHz for the sampling frequency.

For the first scenario, we consider a gaussian channel with varying levels of noise ranging from -6 to 18 dB SNR. For the transmitter, we use 8 samples per symbol and a root raised cosine filter with rolloff 0.35.

For the second scenario, we additionally vary the transmitter settings. We consider either 2, 4, 8 or 16 samples per symbol and the rolloff of the filter can range from 0.15 to 0.45.

For the third and final scenario, we use the transmitter settings of the first scenario, but we consider also Rician and Rayleigh channels with AWGN noise. % Maybe specify the parameters used in both channels...




\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
