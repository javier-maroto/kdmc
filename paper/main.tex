\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


% Added packages
\usepackage{cite}  
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

% Latex math commands
\newcommand{\Ls}{\mathcal{L}}
\def\sP{{\mathbb{P}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%\title{SafeAMC: Adversarial training for robust modulation recognition models}

%\name{Javier Maroto\thanks{This work has been sponsored by armasuisse Science and Technology with the project ROBIN (project code Aramis 047-22).}$^{\star}$ \qquad Gérôme Bovet$^{\dagger}$ \qquad Pascal Frossard$^{\star}$}
  
%\address{$^{\star}$ EPFL, Switzerland \\
%  $^{\dagger}$armasuisse Science\&Technology, Cyber-Defence Campus, Switzerland}
  
\title{Improving modulation classification by distilling maximum likelihood
\thanks{This work has been sponsored by armasuisse Science and Technology with the project ARNO (project code ???).}
}

\author{\IEEEauthorblockN{Javier Maroto}
\IEEEauthorblockA{\textit{Signal Processing Laboratory (LTS4)} \\ \textit{EPFL, Switzerland}}
\and
\IEEEauthorblockN{Gérôme Bovet}
\IEEEauthorblockA{\textit{armasuisse Science\&Technology} \\
\textit{Cyber-Defence Campus, Switzerland}}
\and
\IEEEauthorblockN{Pascal Frossard}
\IEEEauthorblockA{\textit{Signal Processing Laboratory (LTS4)} \\ \textit{EPFL, Switzerland}}}


\begin{document}
\maketitle

\begin{abstract}
In communication systems, there are many tasks, like modulation classification, for which Deep Neural Networks (DNNs) have obtained promising performance. However, these models have been shown to be susceptible to adversarial perturbations, namely imperceptible additive noise crafted to induce misclassification. This raises questions about the security but also about the general trust in model predictions. We propose to use adversarial training, which consists of fine-tuning the model with adversarial perturbations, to increase the robustness of automatic modulation classification (AMC) models. We show that current state-of-the-art models can effectively benefit from adversarial training, which mitigates the robustness issues for some families of modulations. We use adversarial perturbations to visualize the learned features, and we found that the signal symbols are shifted towards the nearest classes in constellation space, like maximum likelihood methods when adversarial training is enabled. This confirms that robust models are not only more secure, but also more interpretable, building their decisions on signal statistics that are actually relevant to modulation classification.
\end{abstract}

\begin{IEEEkeywords}
\end{IEEEkeywords}

\section{Introduction}

% AMC, deep learning is being used. Different solutions
Multiple civil and military applications in communication systems require recognizing with what modulation the information received was encoded. This leads to the use of Automatic Modulation Classification (AMC) on applications ranging from detecting daily radio stations and managing spectrum resources, to eavesdropping and interfering with radio communications. With the emergence of deep learning \cite{goodfellow2016deep}, neural networks made their way into AMC, and are currently the main and most effective method that tackles this problem \cite{OShea_Roy_Clancy_2018}. Compared with previous methods, like feature extraction \cite{xie2008efficient,zhou2013design,zhang2017wireless} or maximum likelihood functions \cite{huan1995likelihood,dobre2007survey,hameed2009likelihood}, neural networks are relatively fast, adaptable to any channel characteristics, and their performance scales well with high quantities of data.

% Effect of adversarial perturbations.
% TODO: maybe add motivation of why robust models are important
%
Recent studies have highlighted that the neural networks used in AMC are susceptible to adversarial attacks \cite{Szegedy_Zaremba_Sutskever_Bruna_Erhan_Goodfellow_Fergus_2014, moosavi2017universal,Sadeghi_Larsson_2019,Lin_Zhao_2020,Flowers_Buehrer_Headley_2019, maroto2021benefits}, in which carefully crafted but almost imperceptible perturbations are added to the transmitted signal. These adversarial perturbations can be used as an effective way of jamming wireless communication systems, requiring much less energy and being stealthier than normal attacks. To craft them, algorithms like FGSM \cite{Goodfellow_Shlens_Szegedy_2015} or PGD \cite{Madry_Makelov_Schmidt_Tsipras_Vladu_2019} are typically used to solve the following optimization:
\begin{equation}
\label{eq:adv_pert}
    \delta_i^* = \argmax_{\delta_i}\Ls(f_{\theta}(x_i + \delta_i), y_i) \quad \text{s.t.} \quad \lVert \delta_i \rVert_{\infty} \leq \varepsilon
\end{equation}
where $\delta_i^*$ is the adversarial perturbation added to the received signal $x_i$, $y_i$ is the one-hot-encoded vector of length K that encodes the modulation used, $\Ls$ is the cross-entropy loss, $f_{\theta}$ is the neural network defined with weights $\theta$, and $\varepsilon$ is a fixed value that constrains the norm of the perturbation to be small and imperceptible.

% Defenses. Relationship with data. Knowledge distillation
Some defenses have been proposed to make networks more robust, like adversarial training \cite{Madry_Makelov_Schmidt_Tsipras_Vladu_2019}, but it comes at a cost in accuracy and computational time. Some works have instead tried to understand why networks are susceptible, and label noise has been pointed as one of the main causes \cite{sanyal2020benign}. This is consistent with the increase in robustness when using knowledge distillation in combination with adversarial training~\cite{goldblum2020adversarially,zi2021revisiting,shao2021and,maroto2022benefits}, essentially training a student model with labels based on the teacher predictions.

% Our work, what are the main motivations. Contributions.
In this work, we propose a data-centric approach with the purpose of making neural networks more accurate and robust to adversarial attacks. Motivated by the success of knowledge distillation, we propose to reduce the label noise in the training data by using the theoretical class probabilities, derived from the noise distribution of the channel with the maximum likelihood function. We show that neural networks trained with these probabilities perform better than if trained with the original labels, confirming the effect of label noise in robustness.


\section{Related Work}

% Maximum likelihood. 
%   (More details can be found in "Survey of automatic modulation classification techniques").
Maximum likelihood \cite{huan1995likelihood,dobre2007survey,hameed2009likelihood} is the analytical solution of the AMC task. It computes the likelihood function of the received signal with all possible modulations and estimates the most likely modulation. While in theory this is Bayes-optimal, it is computationally expensive and requires prior knowledge of the channel characteristics, so sub-optimal approximations are used in practice \cite{dobre2007survey,Hameed_Dobre_Popescu_2009}.

% Recent approaches. Explain and reference.
Deep learning \cite{goodfellow2016deep} has been proposed as a better solution to recognize modulations. %In comparison with the above approaches, it has two clear advantages. It has low complexity which, unlike maximum likelihood methods, scales linearly, and can be trained end-to-end. This increases the quantity and quality of features extracted from the data compared to feature recognition methods. It also allows for extra flexibility, since it does not require any assumption on the channel conditions but learns them directly from the data.
There are mainly two types of Deep Learning models for modulation recognition. On the one hand, inspired by success in speech recognition, some works \cite{Rajendran_Meert_Giustiniano_Lenders_Pollin_2018,Guo_Jiang_Wu_Zhou_2020} propose architectures based on long short-term memory networks (LSTMs) \cite{Hochreiter_Schmidhuber_1997} for modulation recognition. On the other hand, other works \cite{OShea_Corgan_Clancy_2016,West_OShea_2017,Sadeghi_Larsson_2019} employ convolutional neural networks (CNNs) \cite{Krizhevsky_Sutskever_Hinton_2017}, which have been successful in computer vision thanks to their ability to induce translation invariance on the input. The current state-of-the-art in modulation recognition \cite{OShea_Roy_Clancy_2018} uses a model based on the ResNet architecture \cite{Szegedy_Ioffe_Vanhoucke_Alemi_2016}. However, despite their improved performance, there are drawbacks in deep learning approaches compared with the classical methods. In particular, they are black-box models, making them much less interpretable than their classical counterparts, and they are vulnerable to crafted small energy perturbations, which compromise the security of the system against malicious attacks.

% Defenses to adversarial attacks
Adversarial perturbations have also been shown to fool DNNs trained for the task of modulation recognition in a recent work \cite{Sadeghi_Larsson_2019}. The authors propose to make the power of the perturbation dependent on the signal power. They fix the ratio between the signal and the perturbation power, or signal-to-perturbation ratio (SPR), when studying the effect of the perturbations. They test two different scenarios: white-box attacks, where the attacker has access to the model gradients, and black-box attacks, where only the prediction can be accessed. For the white-box setting, they use the Fast Gradient Sign Method (FGSM) \cite{Goodfellow_Shlens_Szegedy_2015} to craft the perturbations, and found that adversarial perturbations require much less power than additive white gaussian noise (AWGN) to fool the network. The authors in \cite{Flowers_Buehrer_Headley_2019} further expand over prior work \cite{Sadeghi_Larsson_2019} using the bit error rate (BER) as a measure of distortion. Additionally, they consider the scenario where the adversarial perturbation is crafted at the transmitter in order to protect the model against eavesdropping. However, in the work they assume perfect knowledge of the DNN used by the eavesdropper, which we think is unrealistic and would fail if there are several eavesdroppers using different modulation recognition models.

% Knowledge distillation
KD consists on training a student model to mimic the outputs of a teacher model. To that end, most KD methods~\cite{hinton2015distilling,romero2014fitnets,zagoruyko2016paying,chebotar2016distilling} append a matching function to the loss which encourages the final representations of the teacher and the student to be close at every sample.
While these methods are effective in transferring clean performance, they are not able to transfer robustness against adversarial examples~\cite{goldblum2020adversarially}. For this reason, recent works have proposed alternatives to address this problem~\cite{goldblum2020adversarially,zi2021revisiting,zhu2021reliable,shao2021and,maroto2022benefits}, that encourage robustness by matching the output of the teacher and student models in the adversarial examples as well.

\section{Maximum likelihood}

Normally, to perform well on automatic modulation classification (AMC), neural networks are trained to infer the modulation used in transmission from the signal received. Thus, the network will be optimized to be as confident and accurate as possible on the training set. However, while enforcing the network to accurately solve the task is feasible for signals with high SNR, on noisier signals typically there is a non-negligible probability that the signal received was generated from a different modulation than the one we are enforcing the model to predict. Such cases are numerous when testing models in the AMC literature, where the signal received could have been generated from any of multiple modulations with similar probability.

While in many other tasks, these probabilities would be difficult or impossible to obtain, in AMC, we can use the maximum likelihood (ML) function to derive the real probabilities. Due to the characteristics of ML, this is only computationally feasible on coherent scenarios, where we have prior knowledge of the channel conditions. Particularly, if we assume the channel is gaussian with variance $\sigma^{2}$, and given the received symbols $r(x)$, where $r$ applies the proper communication filter and downsampling to the received signal, the corresponding ML function is described as
\begin{equation}
	f_{i}(r(x)) = \dfrac{1}{2\pi \sigma^{2}|M_i|}\prod_{t=1}^{T} \sum_{j=1}^{|M_i|} \exp\left(-\dfrac{\lVert r(x)_t - s_j\rVert^{2}}{2 \sigma^{2}}\right)
\end{equation}
where $f_{i}$ is the ML function for a given modulation $M_i$ with $|M_i|$ number of states, $T$ is the number of symbols, $r(x)_t$ is the t-th symbol received, and $s_j$ is the symbol that correspond to the state $j$. Given a set of modulations, the probability of a modulation $M_i$ would be $p_{ML}^{(i)}(x) = \log(f_{i}(r(x))) / \sum_i \log(f_{i}(r(x)))$

The reason why ML is not prefered over neural networks is simple: they perform badly on non-coherent scenarios, where the channel or transmision filter parameters are unknown. Moreover, any small deviation from the real channel parameters has a major effect on the ML performance, which discards the possibility of trying to estimate these parameters. Moreover, incorporating the parameter uncertainty in the ML formulation makes it much more costly computationally without being more performant than neural networks.

Based on these limitations, we propose a hybrid approach. We will use a neural network for inference, but we will train it with the probabilities given by maximum likelihood. This is feasible when we consider that we can have full knowledge of the channel conditions by generating the training samples ourselves. Moreover, after training the neural network, ML is no longer needed. Thus allowing our model to be usable under unknown channel conditions, which is typical in real AMC scenarios.


\section{Knowledge distillation}

\begin{equation}
    \text{ST} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i), y_i)
\end{equation}
where $N$ is the size of the training set, $\Ls$ is the cross-entropy loss, $f_{\theta}$ is the neural network defined with weights $\theta$ and $x_i$ is the signal received. $y_i$ is a one-hot-encoded vector of length K, that encodes the modulation that was used to generate the signal.

\begin{equation}
    \text{ST\_ML} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i), p_{ML}(r(x_i)))
\end{equation}
where $p_{ML}$ is the Maximum Likelihood function, and $r$ is the function that uses the coupled receiver filter and downsamples the signal. Thus, $r$ will be dependent on the filter and samples per symbol used by the transmitter. For this reason, this loss function can only be used when training the network, as this information can be assumed to be known.

\begin{equation}
    \text{ST\_LNR} : \dfrac{1}{|D|}\sum_{i \in D}\Ls(f_{\theta}(x_i), y_i)
\end{equation}
where a training signal $i$ is in the subset $D$ if and only if $\argmax(y_i) = \argmax(p_{ML}(r(x_i)))$. This effectively filters all signals that have label noise. Intuitevely, those signals would be the most corrupted by the noise, to the point where it is more probable that it was generated from a modulation different than the one that was actually used, and potentially confusing the network during training.

\begin{equation}
    \text{AT} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i'), y_i)
\end{equation}
where $x_i' = x_i + \delta_i^*$ is a crafted adversarial signal that maximizes the cross-entropy loss of the neural network in the neighborhood of $x_i$.

\begin{equation}
    \text{AT\_ML} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i'), p_{ML}(r(x_i')))
\end{equation}

\begin{equation}
    \text{AT\_LNR} : \dfrac{1}{|D|}\sum_{i \in D}\Ls(f_{\theta}(x_i'), y_i)
\end{equation}
where a training signal $i$ is in the subset $D$ if and only if $\argmax(y_i) = \argmax(p_{ML}(r(x_i')))$.

In the case where the channel is not gaussian
\begin{equation}
    \text{AT\_NML} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i'), p_{ML}(r(x_i))
\end{equation}

\begin{equation}
    \text{AT\_AML} : \dfrac{1}{N}\sum_{i}\Ls(f_{\theta}(x_i'), p_{ML}(r(x_i)'))
\end{equation}
where $r(x_i)'$ is a crafted adversarial signal that maximizes the cross-entropy loss of maximum likelihood in the neighborhood of $r(x_i)$.



\section{Experiments}

We propose to evaluate our methodology on scenarios with different levels of non-coherence. This way, we can verify if the probabilities generated by maximum likelihood in a coherent scenario are useful to train the model even if it is tested on varying channel and transmitter conditions. 

For all the experiments the task will be the same. We will train the neural network to be able to distinguish between the following digital modulations: BPSK, QPSK, 8/16/32/64/128/256-PSK, PAM4, 16/32/64/128/256-QAM, GFSK, CPFSK, B-FM, DSB-AM, SSB-AM and OQPSK. This is consistent with other experimental settings used in the field \cite{}. We note that due to implementation difficulties, the last six modulations are ignored by maximum likelihood. Thus, the probability of this classes will be one when used, and zero when not.
For all settings, we consider that the signal sampling frequency is 200KHz. We generate 234000 signals for training, and 26000 for testing. The duration of the signal is set to 1024 samples, for consistency with other similar datasets \cite{}. For the network, we use a ResNet-based network architecture \cite{}. We train the network for 100 epochs using SGD optimization with momentum 0.9 and learning rate 0.01, decaying exponentially at a rate of 0.95 per epoch. We use gradient clipping of size 5 and weight decay of 5e-4.

For the first scenario, we consider a gaussian channel with varying levels of noise ranging from -6 to 18 dB SNR. For the transmitter, we use 8 samples per symbol and a root raised cosine filter with rolloff 0.35. The results of using the different methodologies are shown in Table \ref{tab:sbasic}.

For the second scenario, we additionally vary the transmitter settings. We consider either 2, 4, 8 or 16 samples per symbol and the rolloff of the filter can range from 0.15 to 0.45. The results of using the different methodologies are shown in Table \ref{tab:sawgn2p}.

For the third and final scenario, we use the transmitter settings of the first scenario, but we consider also Rician and Rayleigh channels with AWGN noise. The results of using the different methodologies are shown in Table \ref{tab:sp0c20}. % Maybe specify the parameters used in both channels...

\begin{table}[htbp]
	\centering
	%\small
	\begin{tabular}{c|cc}
	    Method & Accuracy & Robustness \\
		\hline
		ST & $85.87 \pm 0.02$ & $19.51 \pm 0.25$ \\ 
		ST\_LNR & $86.34 \pm 0.52$ & $24.61 \pm 1.09$ \\ 
		ST\_ML & $86.59 \pm 0.32$ & $27.30 \pm 0.47$ \\ 
		AT & $57.36 \pm 3.12$ & $54.71 \pm 2.67$ \\ 
		AT\_LNR & $64.50 \pm 0.62$ & $58.44 \pm 0.60$ \\ 
		AT\_ML & $67.01 \pm 1.04$ & $59.44 \pm 0.32$ \\ 
    \end{tabular}
    \caption{Accuracy and robustness for the ResNet model on the first dataset. The adversarial attacks are crafted using $l_{\infty}$ PGD-7 of 20 dB SPR.}
    \label{tab:sbasic}
\end{table}

\begin{table}[htbp]
	\centering
	%\small
	\begin{tabular}{c|cc}
	    Method & Accuracy & Robustness \\
		\hline
		ST & $85.78 \pm 0.10$ & $20.56 \pm 1.14$ \\ 
		ST\_LNR & $86.21 \pm 0.08$ & $22.82 \pm 0.88$ \\ 
		ST\_ML & $\textbf{86.58} \pm 0.22$ & $\textbf{24.08} \pm 2.66$ \\
        \hline
		AT & $59.61 \pm 6.44$ & $55.45 \pm 3.64$ \\ 
		AT\_LNR & $64.53 \pm 1.76$ & $\textbf{59.05} \pm 1.42$ \\ 
		AT\_ML & $\textbf{65.06} \pm 1.77$ & $58.72 \pm 0.83$ \\ 
    \end{tabular}
    \caption{Accuracy and robustness for the ResNet model when varying the transmission settings on a gaussian channel.}
    \label{tab:sawgn2p}
\end{table}

\begin{table}[htbp]
	\centering
	%\small
	\begin{tabular}{c|cc}
	    Method & Accuracy & Robustness \\
		\hline
		ST & $78.34 \pm 0.02$ & $18.86 \pm 1.83$ \\ 
		ST\_LNR & $78.08 \pm 0.04$ & $18.97 \pm 3.03$ \\ 
		ST\_ML & $78.40 \pm 0.22$ & $20.00 \pm 1.61$ \\ 
		AT & $49.00 \pm nan$ & $45.36 \pm nan$ \\ 
		AT\_LNR & $48.16 \pm nan$ & $46.02 \pm nan$ \\ 
		AT\_NML & $39.78 \pm 0.14$ & $36.14 \pm 1.78$ \\ 
		AT\_AML & $53.83 \pm 0.47$ & $47.04 \pm 0.07$ \\ 
    \end{tabular}
    \caption{Accuracy and robustness for the ResNet model on the third dataset. The adversarial attacks are crafted using $l_{\infty}$ PGD-7 of 20 dB SPR.}
    \label{tab:sp0c20}
\end{table}


\section{Conclusion}

% Future work: ML for Rayleigh/Rician channels. Use in other fields like physics. Test with other KD functions. More models.


\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
